{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2 - Train a Nationality Classifier (Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with word vectors\n",
    "Word vectors aren't just used for cool word analogies! They're frequently used for downstream applications in NLP when we need to know something about meaning of words. For example, if we're training a classifier that uses a term-per-dimension representation (like a term-document matrix) and we've only seen the word \"cat\" in our training data but not \"kitty\", then we'll be unable to reason about a new document that has \"kitty\" instead of \"cat\". However, if we use word vectors trained on a much larger corpus (not just the training data), the vectors hopefully encode that \"cat\" and \"kitty\" mean similar things and our model might be able to generalize.\n",
    "\n",
    "In this notebook, we'll try one simple application of word vectors, building on the code you developed in the very first notebook. Here again, we'll train a nationality classifier. However, instead of training on a term-document matrix representation, we'll use the _average word vector_ of the words in the document. This representation has a substantial advantage of being compact - 100 dimensions! - instead of the size of our vocabulary. This dense representation makes for very efficient learning. However, does it make for _effective_ learning? In this notebook you'll get a sense of the performance through different types of tests.\n",
    "\n",
    "The exercises we'll do for 2.2 will be _very_ similar initially to those from week one and will use the same data. However, when we get to creating features, you'll build a wholly different set using dense vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to manually set your random seed when performing machine learning experiments so that they are reproducible for others. Here, we set our seed to 655 to ensure your models and experiments get the expected results when evaluating your homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Let's first read in the corpus file as a `pd.DataFrame`, where each row contains a cleaned-up Wikipedia biography for a person, the name of the person and the nationality of the person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ab6a72f2571787aaac24743a8500a99",
     "grade": false,
     "grade_id": "cell-5dcb4b87e0ed2fc6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio</th>\n",
       "      <th>name</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alain Connes (born 1 April 1947) is a French m...</td>\n",
       "      <td>Alain Connes</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Life\\n=== Early life ===\\nSchopenhauer's birth...</td>\n",
       "      <td>Arthur Schopenhauer</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life and career\\nAlfred Nobel at a young age i...</td>\n",
       "      <td>Alfred Nobel</td>\n",
       "      <td>swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early life\\nAlfred Vogt (both \"Elton\" and \"van...</td>\n",
       "      <td>A. E. van Vogt</td>\n",
       "      <td>canadian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alfons Maria Jakob (2 July 1884 in Aschaffenbu...</td>\n",
       "      <td>Alfons Maria Jakob</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 bio                 name  \\\n",
       "0  Alain Connes (born 1 April 1947) is a French m...         Alain Connes   \n",
       "1  Life\\n=== Early life ===\\nSchopenhauer's birth...  Arthur Schopenhauer   \n",
       "2  Life and career\\nAlfred Nobel at a young age i...         Alfred Nobel   \n",
       "3  Early life\\nAlfred Vogt (both \"Elton\" and \"van...       A. E. van Vogt   \n",
       "4  Alfons Maria Jakob (2 July 1884 in Aschaffenbu...   Alfons Maria Jakob   \n",
       "\n",
       "  nationality  \n",
       "0      french  \n",
       "1      german  \n",
       "2     swedish  \n",
       "3    canadian  \n",
       "4      german  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality_df = pd.read_csv('assets/nationality.tsv.gz', sep='\\t', compression='gzip')\n",
    "nationality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4de5083f92547d7f2ac3351d2832c411",
     "grade": true,
     "grade_id": "cell-d75dd80845cfa23a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the burden on memory, let's grab only the first 75,000 biographies/rows of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_df = nationality_df[:75000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.1: Fix the nationality labels\n",
    "Just as in Assignment 1, we'll fix the nationality labels of the data to clean up noise in Wikipedia's manually labeled entries. Use python's `split()` function to divide these labels when they have a comma and take the last word, which we'll treat as the official national label. \n",
    "\n",
    "*Important note:* Remember that `split` matches exactly what you put in, but there might be variable whitespace around the final token. Use `strip()` to ensure that no nationality has leading or training white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bab2eb2ee3641cecc46894afc88f0ce2",
     "grade": false,
     "grade_id": "cell-10b409afae5416f5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "nationality_df['nationality'] = nationality_df['nationality'].apply(lambda x: x.split(',')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(nationality_df.nationality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e5c2f9443fd5292f20cf39b8fae48d2",
     "grade": true,
     "grade_id": "cell-24d12cf649a0a103",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.2: Filter dataset to only those nationalities with at least 500 occurrences\n",
    "When training any classifier, you need enough examples to learn features that reliably predict the labels. For this homework, let's restrict ourselves to working with only nationalities that have at least 500 occurrences. Create a set called `final_nationalities` that contains only those with at least 500 occurrences. Then, from this restricted label set, let's take the subset of `nationality_df` that use these labels and extract a subset called `cleaned_nationality_df` that holds our final dataset that we'll use for train, test, and development.\n",
    "\n",
    "*Side note:* Often, removing rare labels is another good way of getting rid of noise in our dataset. However, in practice, it's important to check these labels to make sure there are no (or few) systematic errors that would bias your model. Sometimes these biases can have significant real-world impact (e.g., underrepresenting people) and as an ethical data scientist, it's your job to combat the introduction of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4499bdec7d31816e8df167e2608344ce",
     "grade": false,
     "grade_id": "cell-bc95c4287660c353",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "final_nationalities = set()\n",
    "for item, count in Counter(nationality_df.nationality).items():\n",
    "    if count >=500:\n",
    "        final_nationalities.add(item)\n",
    "cleaned_nationality_df = nationality_df[nationality_df['nationality'].isin(final_nationalities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 51931)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_nationalities), len(cleaned_nationality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e0097e0418e5aa20e016760c8117cf1",
     "grade": true,
     "grade_id": "cell-3888d372e5dd087c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.3: Split dataset into test, train and dev\n",
    "We have a large enough dataset that we can effectively split it into train, development, and test sets, using the standard ratio of 80%, 10%, 10% for each, respectively. We'll use `split` from `numpy` to split the data into train, dev, and test separately. We'll call these `train_df`, `dev_df`, and `test_df`.  Note that `split` does not shuffle, so we'll use `DataFrame.sample()` and randomly resample our entire dataset to get a random shuffle before the split.\n",
    "\n",
    "*Important note*: Remember to set  `random_state` in `DataFrame.sample()` to our seed so that you end up with the same (random) ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc98f3d4d03937216c5cc010524de577",
     "grade": false,
     "grade_id": "cell-d41a20cb60219ddb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "cleaned_nationality_df = cleaned_nationality_df.sample(frac=1,random_state=RANDOM_SEED)\n",
    "train_df, dev_df, test_df = np.split(cleaned_nationality_df, [int(0.8 * len(cleaned_nationality_df)), \\\n",
    "                                                              int(0.9 * len(cleaned_nationality_df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.4: print the first instance of your training and your test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71fdc129d9fe3e0bfa881aae9915be70",
     "grade": true,
     "grade_id": "first_instance_train",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early life and World War II\n",
      "Lasky was born in The Bronx of New York City and schooled at City College of New York, where he wrote for the student newspaper, ''The Campus.'' He continued his education at University of Michigan and Columbia University. He briefly considered himself a Trotskyist but at 22 moved away from communism entirely because of disgust with Stalin. He began working for the ''New Leader'' in New York and was editor from 1942–1943. Lasky wrote an editorial during this time criticizing the Allies for failing to address The Holocaust directly in their World War II efforts.\n",
      "\n",
      "He served in World War II as a combat historian for the 7th Army. Lasky remained in Germany after the war, making his home in Berlin, where he worked for American military governor Lucius D. Clay. During this time, Lasky was an outspoken critic of the United States' earlier reluctance to intervene to stop the genocide of European Jews.\n",
      "Other activities and private life\n",
      "Lasky's grave in Berlin\n",
      "\n",
      "Lasky was the author of many books including ''Utopia and Revolution'', ''Voices in the Revolution'', ''On the Barricades and Off'', and ''The Language of Journalism''. He was married twice, to Brigitte Lasky (''née'' Newiger) with whom he had two children, Vivienne Lasky and Oliver Lasky, and to German novelist Helga Hegewisch.\n",
      "\n",
      "Lasky died in May 2004 of a heart ailment. A portion of Lasky's unpublished memoirs appears in ''News from the Republic of Letters'', as well as in ''The Berlin Journal'', Spring, 2007.\n",
      "\n",
      "Giovanni Antonio Burrini (25 April 1656 – 5 January 1727) was a Bolognese painter of Late-Baroque or Rococo style. After an apprenticeship with Domenico Maria Canuti, he went to work under Lorenzo Pasinelli with fellow student, Giovanni Gioseffo dal Sole. He became an early friend and often close collaborator with Giuseppe Maria Crespi, with whom he shared a studio. He became a rival and competitor with Sebastiano Ricci. He painted in Turin for the Carignano family and Novellara. In 1709, he was one of the founding members of the Accademia Clementina in Bologna. \n"
     ]
    }
   ],
   "source": [
    "print(train_df.iloc[0,:]['bio'])\n",
    "print(test_df.iloc[0,:]['bio'])\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying with dense vectors\n",
    "\n",
    "In this exercise we'll be working with dense representations of documents instead of the bag-of-words representations we used earlier. To do this, we'll use the _average_ (or *mean*) word vector of a document and classify from those representations.\n",
    "\n",
    "As a first step, let's tokenize the biographies here using regular expressions like we did in Exercise 2.1. However, since we're going to be computing an average word vector, let's remove stop words. Here, we'll use NLTK's list of English stop words. Since these words shouldn't affect our classification decision, we can remove them to avoid adding any noisy they might cause. Note that all of the stopwords in NLTK's list are lower-cased, but it's possible that some stopwords in your documents are not entirely lower-cased, so they may not match without some further processing.  \n",
    "\n",
    "Your task is to generate a list that contains the list of all non-stopword tokens in each bio of your training set. Call this `tokenized_train_items`. Use the same regular expression you used for Exercise 2.1 to determine tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "852022050df28fd7360294bda1d27c69",
     "grade": false,
     "grade_id": "cell-017d652e392d3e9e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokenize and remove stop words from training data\n",
    "tokenized_train_items = []\n",
    "for bio in train_df['bio']:\n",
    "    # tokenize\n",
    "    tokens = re.findall('\\w+', bio.lower())\n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokenized_train_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tokenized_train_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenized_train_items[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create dense representations from the word2vec models trained on our own wikipedia corpus (`assets/wikipedia.100.word-vecs.kv`). Load it using gensim as `KeyedVectors` and call it `model_wp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d6ada9fc7661d71b44fd3c9e0c8dc92",
     "grade": false,
     "grade_id": "cell-4233a38df1bab1d7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_wp = KeyedVectors.load(\"assets/wikipedia.100.word-vecs.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below that takes in a list of lists of tokens (i.e., those tokenized bios you just made) and a set of word vectors to use. Then for each tokenized document in `tokenized_texts`, it computes the mean word vector of all words in the document. Skip those words that don't exist in the vocabulary of your `word_vectors`.These mean word vectors should be returned as a `numpy array` (i.e., a matrix). If a document has no tokens left after filtering (rare, but happens!), use a vector of all zeros that is equal in length with the vector size of your `word_vectors` as the representation of the document. \n",
    "\n",
    "For more information on numpy’s functions visit: https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8ece59bb15a11f6717e4278b5758379",
     "grade": false,
     "grade_id": "cell-54b0d1df68607c70",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_dense_features(tokenized_texts, word_vectors):\n",
    "    \"\"\"\n",
    "    Generates dense features from tokenized texts using pre-trained word vectors.\n",
    "    \n",
    "    Args:\n",
    "    - tokenized_texts: A list of lists of tokens.\n",
    "    - word_vectors: A set of word vectors to use.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of dense features with shape (len(tokenized_texts), word_vectors.vector_size).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for tokens in tokenized_texts:\n",
    "        valid_tokens = [token for token in tokens if token in word_vectors.vocab]\n",
    "        if len(valid_tokens) > 0:\n",
    "            mean_vec = np.mean([word_vectors[token] for token in valid_tokens], axis=0)\n",
    "            results.append(mean_vec)\n",
    "        else:\n",
    "            results.append(np.zeros(word_vectors.vector_size))\n",
    "    return np.array(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the dense vector representations by calling `generate_dense_features` on the tokenized training data. Let's generate a representation using `model_wp` (i.e., our wikipedia vectors) and call it `X_train_wp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, model_wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.5: Sanity Check: print the shape of X_train_wp\n",
    "Let's ensure that we featurized everything as expected. You should have 100 word features in your Wikipedia training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41544"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a7285d02d9f44ca01a1c7db4f8adb0a",
     "grade": true,
     "grade_id": "shape_x_train",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41544, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_wp.shape)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.6: Get the list of labels\n",
    "We need to get the final list of labels in a python `list` for sklearn to use. Create this list from `train_df` and let's call it `y_train`. `y` (lower case!) is normally used to refer to the label of the classifier (or value in  a regressor) in machine learning. We use the lower case here to indicate it's a vector, whereas `X` is upper case because it's a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "396a445471889d43bb56b57a520af2f4",
     "grade": false,
     "grade_id": "cell-4ea0699c279b9845",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "y_train = list(train_df.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.7: Fit a classifier on a subset of the data\n",
    "\n",
    "Finally, let's fit a classifier on our dense data. For a start we'll use `LogisticRegression`. Don't forget to set the `random_state` to use our `RANDOM_SEED` so you get deterministic (but random) results. To train your classifier, create a `LogisticRegression` object called `clf_wp` and call `fit` with `X_train_wp` and `y_train`. This classifier will get trained on the dense representations we just made.\n",
    "\n",
    "For this cell, let's just use the first 10,000 rows of `X_train_wp` and `y_train` to fit the classifier. In general, when you have a large dataset, it's useful to go end-to-end and train one of these half-baked classifiers to verify that your model works as expected. You can even do some analyses if the performance is good enough to get a sense of how things are working. Then you can train on the full data.\n",
    "\n",
    "*Notes:*\n",
    "1. You should use the `lbfgs` solver, as this generally Just Works™ and is fast.\n",
    "2. Since we have more than two nationalities, we'll set `multi_class='auto'` so that the classifier isn't binary.\n",
    "3. `X_train_wp` is a numpy array, so you'll need to use array indexing operations to get the first 10,000 rows.\n",
    "4. Since we have many classes, we'll increase the maximum number of iterations to 10,000 to ensure convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f76e9bd15f8ca8d6c002a317172dabc",
     "grade": false,
     "grade_id": "cell-c508230490e7fe33",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=655)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_wp = LogisticRegression(random_state=RANDOM_SEED, solver='lbfgs', max_iter=10000, multi_class='auto')\n",
    "clf_wp.fit(X_train_wp[:10000], y_train[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.8: Generate dev data\n",
    "Let's tokenize the dev data so we can create a dense representation. The list `tokenized_dev_items` should contain the list of tokens but exclude all tokens in the nltk stopwords list (just like you did for tokenizing the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "654458cd57efb239c1fb40c6b4b11e15",
     "grade": false,
     "grade_id": "cell-0c302ab446d1d1e3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_dev_items = []\n",
    "for bio in dev_df['bio']:\n",
    "    # tokenize\n",
    "    tokens = re.findall('\\w+', bio.lower())\n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokenized_dev_items.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the dense vector for the dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_wp = generate_dense_features(tokenized_dev_items, model_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b17777feb2e7b52d513e1b578645b337",
     "grade": true,
     "grade_id": "cell-959da6dea68da136",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5193, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_dev_wp.shape)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.9: Create Dummy classifiers\n",
    "It's always important to contextualize your results by comparing it with naive classifiers. If these classifiers do well, then your task is easy! If not, then you can see how much better your system does at first. We'll use two different strategies using the [Dummy Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) class. Create two `DummyClassifier` instances that use the `uniform` (guess randomly) and `most_frequent` strategies and fit these on the training data so we can compare them with our classifier that was trained on 10K instances. In general, you probably always want to at least compare with these two baselines in a classification task.\n",
    "\n",
    "*NOTE:* Be sure to set the `random_state` of the `DummyClassifier` to be `RANDOM_SEED` so your scores match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=655, strategy='most_frequent')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "# Create a Dummy Classifier with uniform strategy\n",
    "clf_dummy_uniform = DummyClassifier(strategy='uniform', random_state=RANDOM_SEED)\n",
    "clf_dummy_uniform.fit(X_train_wp[:10000], y_train[:10000])\n",
    "\n",
    "# Create a Dummy Classifier with most_frequent strategy\n",
    "clf_dummy_most_frequent = DummyClassifier(strategy='most_frequent', random_state=RANDOM_SEED)\n",
    "clf_dummy_most_frequent.fit(X_train_wp[:10000], y_train[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.10: Create logistic regression classifier as in exercise 1.1 that does not use word embedding for comparison\n",
    "\n",
    "As a comparison to see how well our dense-vector-based classifier stacks up, let's create a comparison model that uses words as features (not the vectors). Use the same [TfIdfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) setup as in Exercise 1.1 and the same arguments `min_df=500` and `stop_words='english'`. Create the `TfIdfVectorizer` and call `fit_transform` on the training data to create `X_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5409e4db0eab99dca0b950f4d13f565",
     "grade": false,
     "grade_id": "cell-18311882e1b56c52",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=500, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_df['bio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the bag-of-words classifier on the first 10,000 instances in `X_train` and its labels in `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e9e57a8ca0bb70917fdb3ee2ac32686",
     "grade": false,
     "grade_id": "cell-969e99d8cdde108a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=655)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf = LogisticRegression(solver='lbfgs', multi_class='auto', random_state=RANDOM_SEED)\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', random_state=RANDOM_SEED,max_iter=10000)\n",
    "clf.fit(X_train[:10000], y_train[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the development data in `dev_df` using the vectorizer and call this `X_dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de2abcee0fac9a6767003c3682c26608",
     "grade": false,
     "grade_id": "cell-6084f9d7d2d3fb47",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X_dev = vectorizer.transform(dev_df.bio)\n",
    "y_dev = list(dev_df.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.11: Generate all the predictions\n",
    "\n",
    "Generate predictions for the dense-vector classifier, the bag-of-words classifier and the two dummy classifiers, and store your predictions in the following variables:\n",
    "* `lr_wp_tiny_dev_preds` (dense vector)\n",
    "* `lr_tiny_dev_preds` (bag of words)\n",
    "* `rand_dev_preds` (random baseline)\n",
    "* `mf_dev_preds` (most frequent baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f31466c1ec7c4afe03e9634d65d281e",
     "grade": false,
     "grade_id": "cell-16bd11817c3104b0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# lr_wp_tiny_dev_preds = clf_wp.predict(X_dev_wp)\n",
    "# # lr_wp_tiny_dev_preds = clf_wp.predict_proba(X_dev_wp)[:, 1] > 0.5\n",
    "# lr_tiny_dev_preds = clf.predict(X_dev)\n",
    "# rand_dev_preds = random_clf.predict(X_dev_wp)\n",
    "# mf_dev_preds = mf_clf.predict(X_dev_wp)\n",
    "\n",
    "# Generate predictions for the dense-vector classifier\n",
    "lr_wp_tiny_dev_preds = clf_wp.predict(X_dev_wp)\n",
    "\n",
    "# Generate predictions for the bag-of-words classifier\n",
    "lr_tiny_dev_preds = clf.predict(X_dev)\n",
    "\n",
    "# Generate predictions for the random baseline\n",
    "rand_dev_preds = clf_dummy_uniform.predict(X_dev_wp)\n",
    "\n",
    "# Generate predictions for the most frequent baseline\n",
    "mf_dev_preds = clf_dummy_most_frequent.predict(X_dev_wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.12: Score our predictions\n",
    "Now, let's score the models. Here, we'll use F1 to score and use a _macro_ average so that the score reflects the average F1 performance across all classes. You'll want to define the list of gold standards answers from `dev_df` and call this `y_dev`. Call your f1 scores:\n",
    "* `lr_wp_f1` (dense vector)\n",
    "* `lr_f1` (bag of words)\n",
    "* `rand_f1` (random baseline)\n",
    "* `mf_f1` (most frequent baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['japanese', 'russian', 'british', ..., 'american', 'american',\n",
       "       'british'], dtype='<U13')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_wp_tiny_dev_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "154d1377713a8c73dd3b1ad84cf58ae5",
     "grade": false,
     "grade_id": "cell-8b7ee73cddb0b503",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "lr_wp_f1 = f1_score(y_dev, lr_wp_tiny_dev_preds, average='macro')\n",
    "lr_f1 = f1_score(y_dev, lr_tiny_dev_preds, average='macro')\n",
    "rand_f1 = f1_score(y_dev, rand_dev_preds, average='macro')\n",
    "mf_f1 = f1_score(y_dev, mf_dev_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe5a67152be77f309f247cc12f1b0b35",
     "grade": true,
     "grade_id": "f1_scores_partial",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34071854283097364\n",
      "0.6799816930655451\n",
      "0.042630466943191815\n",
      "0.028832567997174142\n"
     ]
    }
   ],
   "source": [
    "print(lr_wp_f1)\n",
    "print(lr_f1)\n",
    "print(rand_f1)\n",
    "print(mf_f1)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, looking pretty promising. The dense vectors certainly contain some useful information, but the bag of words representation still seems pretty powerful. How about if we trained on all the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.13: Fit a classifier on the full data\n",
    "Train the following classifiers on all the data:\n",
    "* The dense-vector classifier trained on Wikipedia data (assigned to `clf_wp`)\n",
    "* The bag-of-words classifier (assigned to `clf`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8fc93dfa21eef34e40575b3f699069",
     "grade": false,
     "grade_id": "cell-121a1bf40bef0bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=655)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_wp = LogisticRegression(random_state=RANDOM_SEED, solver='lbfgs', max_iter=10000, multi_class='auto')\n",
    "clf_wp.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfcd8e2b6fd1fa7577aff36e0fe28fb8",
     "grade": false,
     "grade_id": "cell-00701158b2edb134",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=655)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', random_state=RANDOM_SEED,max_iter=10000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.14: Generate all the predictions for the final model and score them\n",
    "\n",
    "We'll use the same naming scheme here for reporting F1 scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the dense-vector classifier\n",
    "lr_wp_tiny_dev_preds = clf_wp.predict(X_dev_wp)\n",
    "\n",
    "# Generate predictions for the bag-of-words classifier\n",
    "lr_tiny_dev_preds = clf.predict(X_dev)\n",
    "\n",
    "# Calculate the F1 score for the dense-vector classifier\n",
    "lr_wp_f1 = f1_score(y_dev, lr_wp_tiny_dev_preds, average='macro')\n",
    "\n",
    "# Calculate the F1 score for the bag-of-words classifier\n",
    "lr_f1 = f1_score(y_dev, lr_tiny_dev_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b26f984a91f673e58e1c5ecc8256551",
     "grade": true,
     "grade_id": "f1_scores_full",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3864132452517425\n",
      "0.7442532812711844\n"
     ]
    }
   ],
   "source": [
    "print(lr_wp_f1)\n",
    "print(lr_f1)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some performance improvement! It looks like even a simple model and simple dense representation is still able to capture a lot of information (especially when compared with the baselines). That said, why might the bag of words model be doing so well? Thing about what kinds of features we see in text. Do we need dense representations for these (which help with generalization)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "So far, we've only used very simple representations and simple classifiers. If you're interested, you can try some of the following:\n",
    "* Try using a classifier that can look at combinations of features like a Multi-layer Perception or Random Forest. Since the dense vectors have fewer features, these models can be _much_ more efficient to train than compared to using a bag of words classifier.\n",
    "* Try training your own vectors (or finding other vectors online!) and see if you can get higher performance\n",
    "* So far, we've represented a biography as just an average word vector. What if we wanted to up-weight certain words? One idea is to use TF-IDF to decide how to combine word vectors, so more important/rare words are more heavily weighted. Try adding in this weighting to see if it improves performance.\n",
    "\n",
    "If you try any of these, feel free to discuss them on the class's Slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_applied_natural_language_processing_v2_assignment2_part2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
